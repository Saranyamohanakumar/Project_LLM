{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install python-docx\n",
        "!pip install PyPDF2"
      ],
      "metadata": {
        "id": "sJpDCMPCFmeN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rm -rf ~/.cache/huggingface/hub\n"
      ],
      "metadata": {
        "id": "dApBHkY564I4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n"
      ],
      "metadata": {
        "id": "9dkb2tqm69jL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymilvus"
      ],
      "metadata": {
        "id": "LVwtrH8orX2s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import PyPDF2, docx, json, random\n",
        "from transformers import pipeline\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from pymilvus import connections, Collection\n",
        "\n",
        "\n",
        "\n",
        "# Connect to Zilliz Cloud\n",
        "\n",
        "connections.connect(\n",
        "    alias=\"default\",\n",
        "    uri=\"https://in03-feb569ec82b1b76.serverless.aws-eu-central-1.cloud.zilliz.com\",  # your URI\n",
        "    token=\"520e0e883ef97fcc4663dca8514090a2a491dd29b714711e961807fdbff8a163d82abd308cf1a8ef546698eb1759aa7054cbc295\")\n",
        "\n",
        "\n",
        "collection = Collection(\"interveiw_Knowledge\")\n",
        "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n"
      ],
      "metadata": {
        "id": "Im0ib-nvRF5z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Load LLM model\n",
        "\n",
        "# Load LLM (small one for demo)\n",
        "chatbot_model = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=\"HuggingFaceH4/zephyr-7b-beta\",\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=\"auto\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "uIirWN47np5d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IQC1clNxnB2H"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "import PyPDF2, docx, json, random\n",
        "from transformers import pipeline\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from pymilvus import connections, Collection\n",
        "import random\n",
        "\n",
        "# Store sessions for mock interview\n",
        "interview_sessions = {}\n",
        "\n",
        "\n",
        "# Step 3Ô∏è: File Extractors\n",
        "\n",
        "def extract_text_from_pdf(file_path):\n",
        "    text = \"\"\n",
        "    with open(file_path, \"rb\") as f:\n",
        "        reader = PyPDF2.PdfReader(f)\n",
        "        for page in reader.pages:\n",
        "            page_text = page.extract_text()\n",
        "            if page_text:\n",
        "                text += page_text + \"\\n\"\n",
        "    return text.strip()\n",
        "\n",
        "\n",
        "def extract_text_from_docx(file_path):\n",
        "    doc = docx.Document(file_path)\n",
        "    return \"\\n\".join([p.text for p in doc.paragraphs if p.text.strip()])\n",
        "\n",
        "\n",
        "def extract_text(file_path):\n",
        "    if file_path.endswith(\".pdf\"):\n",
        "        return extract_text_from_pdf(file_path)\n",
        "    elif file_path.endswith(\".docx\"):\n",
        "        return extract_text_from_docx(file_path)\n",
        "    return \"\"\n",
        "\n",
        "\n",
        "\n",
        "# Step 4Ô∏è: RAG Retrieval Function\n",
        "\n",
        "def retrieve_questions_from_zilliz(cv_text, jd_text, top_k=5):\n",
        "    # If user uploads blank or short files\n",
        "    if not cv_text.strip() or not jd_text.strip():\n",
        "        return get_random_questions_from_collection(top_k)\n",
        "\n",
        "    # Create a meaningful semantic search query\n",
        "    query = f\"{jd_text[:500]} related interview questions for candidate with {cv_text[:500]}\"\n",
        "    query_vector = embedder.encode([query]).tolist()\n",
        "\n",
        "    search_params = {\"metric_type\": \"COSINE\", \"params\": {\"nprobe\": 10}}\n",
        "\n",
        "    results = collection.search(\n",
        "        data=query_vector,\n",
        "        anns_field=\"embedding\",\n",
        "        param=search_params,\n",
        "        limit=50,\n",
        "        output_fields=[\"text\"]\n",
        "    )\n",
        "\n",
        "    # Extract retrieved question texts\n",
        "    hits = [hit.entity.get(\"text\") for hit in results[0] if hit.distance > 0]\n",
        "\n",
        "    # Remove duplicates while preserving order\n",
        "    hits = list(dict.fromkeys(hits))\n",
        "\n",
        "    # Fallback if no results found\n",
        "    if not hits:\n",
        "        hits = get_random_questions_from_collection(top_k)\n",
        "\n",
        "    # Randomize subset\n",
        "    hits = random.sample(hits, min(top_k, len(hits)))\n",
        "    return hits\n",
        "\n",
        "\n",
        "def get_random_questions_from_collection(n=5):\n",
        "    \"\"\"Fetch random questions from the whole collection.\"\"\"\n",
        "    data = collection.query(expr=\"\", output_fields=[\"text\"], limit=100)\n",
        "    all_qs = [d[\"text\"] for d in data]\n",
        "    return random.sample(all_qs, min(n, len(all_qs)))\n",
        "\n",
        "\n",
        "# Step 5Ô∏è: Generate Summary + Questions\n",
        "\n",
        "def generate_summary_and_questions(cv_file, jd_file):\n",
        "    if not cv_file or not jd_file:\n",
        "        return \"‚ö†Ô∏è Please upload both a CV and Job Description.\", []\n",
        "\n",
        "    cv_text = extract_text(cv_file)\n",
        "    jd_text = extract_text(jd_file)\n",
        "\n",
        "    questions = retrieve_questions_from_zilliz(cv_text, jd_text, top_k=5)\n",
        "\n",
        "    if not questions:\n",
        "        return \"No relevant questions found. Try a different job description.\", []\n",
        "\n",
        "    summary = (\n",
        "        f\"üéØ **Top {len(questions)} relevant questions retrieved from knowledge base:**\\n\\n\"\n",
        "        + \"\\n\".join([f\"{i+1}. {q}\" for i, q in enumerate(questions)])\n",
        "    )\n",
        "    return summary, questions\n",
        "\n",
        "\n",
        "\n",
        "# Step 6Ô∏è:  Feedback Generator\n",
        "\n",
        "def give_feedback(answer, question):\n",
        "    prompt = (\n",
        "        f\"Evaluate the following candidate answer to a technical question.\\n\\n\"\n",
        "        f\"Question: {question}\\n\"\n",
        "        f\"Candidate's Answer: {answer}\\n\\n\"\n",
        "        f\"Your response must include only:\\n\"\n",
        "        f\"1Ô∏è‚É£ Evaluation: State 'Correct', 'Partially Correct', or 'Incorrect'.\\n\"\n",
        "        f\"2Ô∏è‚É£ Correct Answer: Write the ideal answer in 1‚Äì2 short sentences.\\n\"\n",
        "        f\"3Ô∏è‚É£ Review:\\n\"\n",
        "        f\"   - ‚úÖ Strengths (1 short sentence)\\n\"\n",
        "        f\"   - ‚ö†Ô∏è Areas for improvement (1 short sentence)\\n\\n\"\n",
        "        f\"Keep your total response under 100 words. Do NOT show any examples or repeat the question.\"\n",
        "    )\n",
        "\n",
        "    response = chatbot_model(\n",
        "        prompt,\n",
        "        max_new_tokens=150,\n",
        "        do_sample=False,\n",
        "        temperature=0.3,\n",
        "        top_p=0.9\n",
        "    )[0][\"generated_text\"]\n",
        "\n",
        "    return response.replace(prompt, \"\").strip()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Step 7Ô∏è: Mock Interview Logic (Final Fixed Version)\n",
        "\n",
        "def start_interview(questions):\n",
        "    \"\"\"Start the interview and initialize session.\"\"\"\n",
        "    if not questions:\n",
        "        return \"‚ö†Ô∏è No questions available. Please upload CV + JD first.\", {}\n",
        "\n",
        "    session_data = {\"questions\": questions, \"current\": 0}\n",
        "    first_q = questions[0]\n",
        "    status_msg = f\"üé§ **Starting personalized mock interview!**\\n\\nüß† **First question:**\\n{first_q}\"\n",
        "    return status_msg, session_data\n",
        "\n",
        "\n",
        "def chat_with_bot(message, history, session_data):\n",
        "    \"\"\"Handle user responses and provide feedback.\"\"\"\n",
        "    if not session_data or \"questions\" not in session_data or not session_data[\"questions\"]:\n",
        "        return \"‚ö†Ô∏è Please start the mock interview first.\", session_data\n",
        "\n",
        "    q_index = session_data.get(\"current\", 0)\n",
        "    questions = session_data[\"questions\"]\n",
        "\n",
        "    current_question = questions[q_index]\n",
        "    feedback = give_feedback(message, current_question)\n",
        "\n",
        "    q_index += 1\n",
        "    if q_index < len(questions):\n",
        "        session_data[\"current\"] = q_index\n",
        "        next_q = questions[q_index]\n",
        "        progress = f\"({q_index}/{len(questions)})\"\n",
        "        reply = f\"üí¨ **Feedback:** {feedback}\\n\\n‚û°Ô∏è **Next question {progress}:** {next_q}\"\n",
        "    else:\n",
        "        reply = f\"üí¨ **Final Feedback:** {feedback}\\n\\n‚úÖ Interview finished. Great job!\"\n",
        "        session_data = {}  # reset after interview\n",
        "\n",
        "    return reply, session_data\n",
        "\n",
        "\n",
        "\n",
        "# Step 8Ô∏è: Gradio Interface\n",
        "\n",
        "with gr.Blocks(theme=gr.themes.Soft(), title=\"RAG Mock Interview Coach\") as demo:\n",
        "    gr.Markdown(\"## ü§ñ CV + JD Powered Mock Interview Coach\")\n",
        "\n",
        "    with gr.Tabs():\n",
        "        # Tab 1: Upload & Analyze\n",
        "        with gr.Tab(\"Upload & Analyze\"):\n",
        "            gr.Markdown(\"### üìÑ Upload your CV and Job Description to generate relevant questions.\")\n",
        "            with gr.Row():\n",
        "                cv_upload = gr.File(label=\"Upload CV (PDF or DOCX)\", type=\"filepath\")\n",
        "                jd_upload = gr.File(label=\"Upload Job Description (PDF or DOCX)\", type=\"filepath\")\n",
        "\n",
        "            output = gr.Markdown(label=\"Extracted Info & Questions\")\n",
        "            role_questions = gr.State([])\n",
        "\n",
        "            generate_btn = gr.Button(\"üîç Analyze CV + JD & Retrieve Questions\")\n",
        "            generate_btn.click(\n",
        "                fn=generate_summary_and_questions,\n",
        "                inputs=[cv_upload, jd_upload],\n",
        "                outputs=[output, role_questions]\n",
        "            )\n",
        "\n",
        "        # Tab 2: Mock Interview\n",
        "        with gr.Tab(\"Mock Interview\"):\n",
        "            gr.Markdown(\"### üé§ Start Your Personalized Mock Interview\")\n",
        "\n",
        "            # Session state to persist across chat\n",
        "            session_state = gr.State({})\n",
        "\n",
        "            start_btn = gr.Button(\"üöÄ Start Mock Interview\")\n",
        "            start_output = gr.Markdown(label=\"Interview Status\")\n",
        "\n",
        "            # Start interview properly ‚Äî split text + state\n",
        "            def start_and_show(questions):\n",
        "                message, session = start_interview(questions)\n",
        "                return message, session\n",
        "\n",
        "            start_btn.click(\n",
        "                fn=start_and_show,\n",
        "                inputs=[role_questions],\n",
        "                outputs=[start_output, session_state]\n",
        "            )\n",
        "\n",
        "            chatbot = gr.ChatInterface(\n",
        "                fn=chat_with_bot,\n",
        "                title=\"Interview Coach\",\n",
        "                description=\"üí¨ Answer the question, and get instant feedback!\",\n",
        "                additional_inputs=[session_state],\n",
        "                additional_outputs=[session_state]\n",
        "            )\n",
        "\n",
        "demo.launch()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building RAG"
      ],
      "metadata": {
        "id": "ugbeurCvaLqj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"in03-feb569ec82b1b76.serverless.aws-eu-central-1.cloud.zilliz.com\"\n",
        "token = \"520e0e883ef97fcc4663dca8514090a2a491dd29b714711e961807fdbff8a163d82abd308cf1a8ef546698eb1759aa7054cbc295\""
      ],
      "metadata": {
        "id": "Lf3VTu1a9arJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pymilvus import connections\n",
        "\n",
        "connections.connect(\n",
        "    alias=\"default\",\n",
        "    uri=\"https://in03-feb569ec82b1b76.serverless.aws-eu-central-1.cloud.zilliz.com\",\n",
        "    token=\"520e0e883ef97fcc4663dca8514090a2a491dd29b714711e961807fdbff8a163d82abd308cf1a8ef546698eb1759aa7054cbc295\"\n",
        ")"
      ],
      "metadata": {
        "id": "fqPRBXLZ-F2B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# upload the CSV file into the notebook\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "8N4wHYI7KgP5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pymilvus import Collection, connections\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import json\n",
        "\n",
        "\n",
        "\n",
        "# Load Collection\n",
        "\n",
        "collection = Collection(\"interveiw_Knowledge\")\n",
        "\n",
        "\n",
        "# Load JSON Data\n",
        "\n",
        "with open(\"interview_data.json\", \"r\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "\n",
        "# Create Texts + Embeddings\n",
        "\n",
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "texts = [f\"{item['role']} - {item['skill']} - {item['question']}\" for item in data]\n",
        "embeddings = model.encode(texts).tolist()\n",
        "\n",
        "print(f\"Embedding dimension: {len(embeddings[0])}\")\n",
        "\n",
        "# Insert (ORDER MATTERS!)\n",
        "\n",
        "entities = [texts, embeddings]  # text first, then embedding\n",
        "insert_result = collection.insert(entities)\n",
        "collection.flush()\n",
        "\n",
        "print(f\"‚úÖ Inserted {len(texts)} records successfully!\")\n",
        "print(\"Inserted IDs:\", insert_result.primary_keys)\n",
        "\n",
        "\n",
        "# Verify entity count\n",
        "\n",
        "print(\"Total entities in collection:\", collection.num_entities)\n"
      ],
      "metadata": {
        "id": "hAIBqIdg_yF8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Explain the difference between supervised and unsupervised learning.\"\n",
        "query_vector = model.encode([query]).tolist()\n",
        "\n",
        "search_params = {\"metric_type\": \"COSINE\", \"params\": {\"nprobe\": 10}}\n",
        "results = collection.search(\n",
        "    data=query_vector,\n",
        "    anns_field=\"embedding\",\n",
        "    param=search_params,\n",
        "    limit=3,\n",
        "    output_fields=[\"text\"]\n",
        ")\n",
        "\n",
        "print(\"\\nüîç Top matches:\")\n",
        "for hit in results[0]:\n",
        "    print(f\"Score: {hit.distance:.4f} | Text: {hit.entity.get('text')}\")\n"
      ],
      "metadata": {
        "id": "AIdh7C-HPosv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from pymilvus import Collection, connections\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "\n",
        "# Connect to Zilliz Cloud\n",
        "\n",
        "connections.connect(\n",
        "    alias=\"default\",\n",
        "    uri=\"https://in03-feb569ec82b1b76.serverless.aws-eu-central-1.cloud.zilliz.com\",\n",
        "    token=\"520e0e883ef97fcc4663dca8514090a2a491dd29b714711e961807fdbff8a163d82abd308cf1a8ef546698eb1759aa7054cbc295\"\n",
        ")\n",
        "\n",
        "\n",
        "# Load Existing Collection\n",
        "\n",
        "collection = Collection(\"interveiw_Knowledge\")\n",
        "\n",
        "\n",
        "# Load CSV Data\n",
        "\n",
        "df = pd.read_csv(\"interview_dataset_enhanced.csv\")\n",
        "\n",
        "print(f\"Loaded {len(df)} rows from CSV\")\n",
        "\n",
        "\n",
        "# Create Combined Texts\n",
        "\n",
        "texts = [f\"{row.role} - {row.skill} - {row.question}\" for _, row in df.iterrows()]\n",
        "\n",
        "\n",
        "# Create Embeddings\n",
        "\n",
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "embeddings = model.encode(texts).tolist()\n",
        "\n",
        "print(f\"‚úÖ Embedding dimension: {len(embeddings[0])}\")\n",
        "\n",
        "\n",
        "# Insert into Collection\n",
        "\n",
        "entities = [texts, embeddings]  # Must match your schema order\n",
        "insert_result = collection.insert(entities)\n",
        "collection.flush()\n",
        "\n",
        "print(f\"‚úÖ Inserted {len(texts)} records successfully!\")\n",
        "print(\"Inserted IDs:\", insert_result.primary_keys)\n",
        "\n",
        "\n",
        "# Verify Entity Count\n",
        "\n",
        "print(\"Total entities in collection:\", collection.num_entities)\n"
      ],
      "metadata": {
        "id": "2as2vZFX9P1k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pymilvus import Collection, connections\n",
        "\n",
        "connections.connect(\n",
        "    alias=\"default\",\n",
        "    uri=\"https://in03-feb569ec82b1b76.serverless.aws-eu-central-1.cloud.zilliz.com\",\n",
        "    token=\"520e0e883ef97fcc4663dca8514090a2a491dd29b714711e961807fdbff8a163d82abd308cf1a8ef546698eb1759aa7054cbc295\"\n",
        ")\n",
        "\n",
        "collection = Collection(\"interveiw_Knowledge\")\n",
        "collection.load()\n",
        "\n",
        "print(collection.schema)  # üëà check field names + dimensions\n",
        "\n",
        "# Preview 5 entries\n",
        "results = collection.query(expr=\"\", output_fields=[\"text\"], limit=5)\n",
        "for r in results:\n",
        "    print(r[\"text\"])\n"
      ],
      "metadata": {
        "id": "Cuhjpt9JCT87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Retrieval Evaluation (Accuracy, Precision, Recall)bold text"
      ],
      "metadata": {
        "id": "guewajyM_0Xh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_cases = [\n",
        "    {\n",
        "        \"cv\": \"Experienced in SQL and data integration using ETL tools.\",\n",
        "        \"jd\": \"Looking for a Data Engineer with expertise in SQL joins and ETL pipelines.\",\n",
        "        \"expected\": [\n",
        "            \"Data Engineer - SQL - Can you explain the difference between INNER JOIN and LEFT JOIN in SQL?\",\n",
        "            \"Data Engineer - ETL - What is the purpose of an ETL pipeline?\",\n",
        "            \"Data Engineer - ETL - How do you handle duplicate data in ETL pipelines?\"\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"cv\": \"Skilled in Python and data cleaning with pandas.\",\n",
        "        \"jd\": \"Hiring a Data Engineer who can handle missing values and large datasets in Python.\",\n",
        "        \"expected\": [\n",
        "            \"Data Engineer - Python - How would you handle missing values in a large dataset using Python?\",\n",
        "            \"Data Engineer - Python - How do you read a large CSV file efficiently in Python?\",\n",
        "            \"Data Engineer - Python - How do you optimize Pandas operations for large datasets?\"\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"cv\": \"Worked on Hadoop and Spark for distributed big data processing.\",\n",
        "        \"jd\": \"Seeking a Data Engineer skilled in Hadoop, Spark, and distributed computing.\",\n",
        "        \"expected\": [\n",
        "            \"Data Engineer - Big Data - Can you explain the difference between Hadoop and Spark?\",\n",
        "            \"Data Engineer - Big Data - What are the main components of Hadoop?\",\n",
        "            \"Data Engineer - Big Data - What is partitioning in Spark?\"\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"cv\": \"Experienced with cloud services and data warehousing.\",\n",
        "        \"jd\": \"Looking for a Data Engineer experienced with AWS and Redshift.\",\n",
        "        \"expected\": [\n",
        "            \"Data Engineer - Cloud - How do you set up data pipelines in AWS?\",\n",
        "            \"Data Engineer - Cloud - What is the difference between AWS Redshift and Snowflake?\",\n",
        "            \"Data Engineer - Cloud - What is autoscaling in cloud services?\"\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"cv\": \"Worked on model deployment pipelines and automation.\",\n",
        "        \"jd\": \"Hiring an ML Engineer with MLOps and CI/CD experience.\",\n",
        "        \"expected\": [\n",
        "            \"ML Engineer - MLOps - What is CI/CD in the context of ML?\",\n",
        "            \"ML Engineer - Deployment - What is containerization and why is it useful for ML models?\",\n",
        "            \"ML Engineer - MLOps - What is a feature store in MLOps?\"\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"cv\": \"Strong knowledge of regression and machine learning evaluation.\",\n",
        "        \"jd\": \"Looking for a Data Scientist experienced in model metrics and overfitting.\",\n",
        "        \"expected\": [\n",
        "            \"Data Scientist - Model Evaluation - What metrics would you use for a regression model?\",\n",
        "            \"Data Scientist - Machine Learning - What is overfitting in machine learning?\",\n",
        "            \"Data Scientist - Machine Learning - What is the bias-variance tradeoff?\"\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"cv\": \"Experience with data visualization and dashboarding.\",\n",
        "        \"jd\": \"Hiring a Data Analyst with expertise in visualization tools.\",\n",
        "        \"expected\": [\n",
        "            \"Data Analyst - Visualization - What are some best practices for designing dashboards?\",\n",
        "            \"Data Analyst - Visualization - What are KPIs and how do you visualize them?\",\n",
        "            \"Data Analyst - Visualization - What is the difference between bar chart and histogram?\"\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"cv\": \"Experience in deep learning and optimization algorithms.\",\n",
        "        \"jd\": \"Seeking an ML Engineer experienced in optimization and training stability.\",\n",
        "        \"expected\": [\n",
        "            \"ML Engineer - Optimization - What is gradient descent?\",\n",
        "            \"ML Engineer - Optimization - What is gradient clipping?\",\n",
        "            \"ML Engineer - Optimization - What is hyperparameter tuning?\"\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"cv\": \"Skilled in SQL query optimization and performance tuning.\",\n",
        "        \"jd\": \"Looking for a Data Engineer who can work with complex SQL queries.\",\n",
        "        \"expected\": [\n",
        "            \"Data Engineer - SQL - What is the difference between UNION and UNION ALL in SQL?\",\n",
        "            \"Data Engineer - SQL - What is the difference between HAVING and WHERE in SQL?\",\n",
        "            \"Data Engineer - SQL - What are window functions used for in SQL?\"\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"cv\": \"Knowledge of statistics, hypothesis testing, and probability.\",\n",
        "        \"jd\": \"Hiring a Data Scientist with strong statistical background.\",\n",
        "        \"expected\": [\n",
        "            \"Data Scientist - Statistics - What is p-value in hypothesis testing?\",\n",
        "            \"Data Scientist - Statistics - What is the central limit theorem?\",\n",
        "            \"Data Scientist - Statistics - What is the difference between Type I and Type II error?\"\n",
        "        ]\n",
        "    }\n",
        "]\n"
      ],
      "metadata": {
        "id": "1tLDRwoF_z74"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from pymilvus import connections, Collection\n",
        "\n",
        "# Connect to Zilliz Cloud\n",
        "connections.connect(\n",
        "    alias=\"default\",\n",
        "    uri= \"https://in03-feb569ec82b1b76.serverless.aws-eu-central-1.cloud.zilliz.com\",\n",
        "    token=\"520e0e883ef97fcc4663dca8514090a2a491dd29b714711e961807fdbff8a163d82abd308cf1a8ef546698eb1759aa7054cbc295\"\n",
        ")\n",
        "\n",
        "collection = Collection(\"interveiw_Knowledge\")\n",
        "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# Helper function to get retrieved questions\n",
        "def retrieve_questions_from_zilliz(cv_text, jd_text, top_k=5):\n",
        "    query = f\"{jd_text[:500]} related interview questions for candidate with {cv_text[:500]}\"\n",
        "    query_vector = embedder.encode([query]).tolist()\n",
        "    search_params = {\"metric_type\": \"COSINE\", \"params\": {\"nprobe\": 10}}\n",
        "    results = collection.search(\n",
        "        data=query_vector,\n",
        "        anns_field=\"embedding\",\n",
        "        param=search_params,\n",
        "        limit=top_k,\n",
        "        output_fields=[\"text\"]\n",
        "    )\n",
        "    hits = [hit.entity.get(\"text\") for hit in results[0]]\n",
        "    return hits\n"
      ],
      "metadata": {
        "id": "D4vx8necAku7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "def bleu_evaluate(predicted, actual):\n",
        "    smoothie = SmoothingFunction().method4\n",
        "    scores = []\n",
        "    for p in predicted:\n",
        "        for a in actual:\n",
        "            reference = [a.split()]\n",
        "            candidate = p.split()\n",
        "            score = sentence_bleu(reference, candidate, smoothing_function=smoothie)\n",
        "            scores.append(score)\n",
        "    return sum(scores) / len(scores) if scores else 0"
      ],
      "metadata": {
        "id": "nmuBvZuOzrlp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_retrieval(predicted, actual):\n",
        "    predicted_set = set(predicted)\n",
        "    actual_set = set(actual)\n",
        "    true_positives = len(predicted_set & actual_set)\n",
        "    precision = true_positives / len(predicted_set) if predicted_set else 0\n",
        "    recall = true_positives / len(actual_set) if actual_set else 0\n",
        "    accuracy = true_positives / max(len(predicted_set | actual_set), 1)\n",
        "    return precision, recall, accuracy\n"
      ],
      "metadata": {
        "id": "YJN5Vk_bAyMm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation to Measure Semantic Similarity"
      ],
      "metadata": {
        "id": "8C5BCktc2EJu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import util\n",
        "\n",
        "def semantic_evaluate(predicted, actual, model, threshold=0.5):\n",
        "    \"\"\"Compute semantic similarity (average cosine) between retrieved and expected questions.\"\"\"\n",
        "    total_score = 0\n",
        "    count = 0\n",
        "    for p in predicted:\n",
        "        for a in actual:\n",
        "            sim = util.cos_sim(model.encode(p), model.encode(a)).item()\n",
        "            total_score += sim\n",
        "            count += 1\n",
        "    avg_similarity = total_score / count if count else 0\n",
        "    return avg_similarity\n",
        "\n",
        "# Example usage:\n",
        "#similarities = []\n",
        "#results1 = []\n",
        "#for i, case in enumerate(test_cases):\n",
        "    #retrieved = retrieve_questions_from_zilliz(case[\"cv\"], case[\"jd\"], top_k=5)\n",
        "    #sim = semantic_evaluate(retrieved, case[\"expected\"], embedder)\n",
        "    ##results1.append({\"case\": case[\"jd\"][:50], \"avg_similarity\": round(sim, 3)})\n",
        "\n",
        "##import pandas as pd\n",
        "#df = pd.DataFrame(results1)\n",
        "#display(df)\n",
        "#print(\"Mean Semantic Similarity:\", round(df.avg_similarity.mean(), 3))"
      ],
      "metadata": {
        "id": "AcHMh4FL2DhO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = []\n",
        "for case in test_cases:\n",
        "    retrieved = retrieve_questions_from_zilliz(case[\"cv\"], case[\"jd\"], top_k=5)\n",
        "    precision, recall, accuracy = evaluate_retrieval(retrieved, case[\"expected\"])\n",
        "    sim = semantic_evaluate(retrieved, case[\"expected\"], embedder)\n",
        "    # BLEU score\n",
        "    bleu = bleu_evaluate(retrieved, case[\"expected\"])\n",
        "    results.append({\n",
        "        \"case\": case[\"jd\"][:40] + \"...\",\n",
        "        \"precision\": round(precision, 2),\n",
        "        \"recall\": round(recall, 2),\n",
        "        \"accuracy\": round(accuracy, 2),\n",
        "        \"BLEU\": round(bleu, 3),\n",
        "        \"Semantic Similarity\": round(sim, 3)\n",
        "    })\n",
        "\n",
        "import pandas as pd\n",
        "evaluation_df = pd.DataFrame(results)\n",
        "print(evaluation_df)\n",
        "print(\"Mean Precision:\", evaluation_df.precision.mean())\n",
        "print(\"Mean Recall:\", evaluation_df.recall.mean())\n",
        "print(\"Mean Accuracy:\", evaluation_df.accuracy.mean())\n",
        "print(\"Mean Semantic Similarity:\", evaluation_df[\"Semantic Similarity\"].mean())\n",
        "print(\"Mean BLEU:\", evaluation_df.BLEU.mean())\n",
        "\n",
        "display(evaluation_df)"
      ],
      "metadata": {
        "id": "zMdesu9WA1li"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.barh(df[\"case\"], df[\"avg_similarity\"], color=\"skyblue\")\n",
        "plt.xlabel(\"Average Semantic Similarity\")\n",
        "plt.title(\"Retrieval Performance by Job Role\")\n",
        "plt.gca().invert_yaxis()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "MKqKbDm9IDen"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8,5))\n",
        "plt.barh(df[\"case\"], evaluation_df[\"BLEU\"], color=\"lightgreen\")\n",
        "plt.xlabel(\"BLEU Score\")\n",
        "plt.title(\"BLEU Score by Job Role\")\n",
        "plt.gca().invert_yaxis()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EPAtW1_u4o3k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cosine Similarity"
      ],
      "metadata": {
        "id": "qg7bupORRxz6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import util\n",
        "def evaluate_cosine_similarity(predicted, actual, model):\n",
        "    \"\"\"Compute average cosine similarity between retrieved and expected questions.\"\"\"\n",
        "    if not predicted or not actual:\n",
        "        return 0.0\n",
        "\n",
        "    total_similarity = 0\n",
        "    count = 0\n",
        "\n",
        "    for p in predicted:\n",
        "        p_emb = model.encode(p, convert_to_tensor=True)\n",
        "        for a in actual:\n",
        "            a_emb = model.encode(a, convert_to_tensor=True)\n",
        "            sim = util.cos_sim(p_emb, a_emb).item()\n",
        "            total_similarity += sim\n",
        "            count += 1\n",
        "\n",
        "    return total_similarity / count\n"
      ],
      "metadata": {
        "id": "RatuMkuKRru7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cosine_results = []\n",
        "\n",
        "for i, case in enumerate(test_cases):\n",
        "    retrieved = retrieve_questions_from_zilliz(case[\"cv\"], case[\"jd\"], top_k=5)\n",
        "    avg_cosine = evaluate_cosine_similarity(retrieved, case[\"expected\"], embedder)\n",
        "    cosine_results.append({\n",
        "        \"case\": case[\"jd\"][:50] + \"...\",\n",
        "        \"avg_cosine_similarity\": round(avg_cosine, 3)\n",
        "    })\n",
        "    print(f\"üîπ Case {i+1} - Avg Cosine Similarity: {round(avg_cosine, 3)}\")\n",
        "\n",
        "import pandas as pd\n",
        "df_cosine = pd.DataFrame(cosine_results)\n",
        "display(df_cosine)\n",
        "\n",
        "print(\"Mean Cosine Similarity:\", round(df_cosine.avg_cosine_similarity.mean(), 3))\n"
      ],
      "metadata": {
        "id": "04mcH8BJR1Qh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10,4))\n",
        "plt.bar(df_cosine.index, df_cosine.avg_cosine_similarity, color='skyblue')\n",
        "plt.xticks(df_cosine.index, [f\"Case {i+1}\" for i in df_cosine.index], rotation=45)\n",
        "plt.ylabel(\"Cosine Similarity\")\n",
        "plt.title(\"Semantic Similarity between Retrieved and Expected Questions\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "mHXLysm-R8l9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Data\n",
        "values = df_cosine.avg_cosine_similarity\n",
        "cases = [f\"Case {i+1}\" for i in df_cosine.index]\n",
        "mean_val = values.mean()\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10,5))\n",
        "bars = plt.bar(cases, values, color=\"#4fa3d1\", edgecolor=\"black\", alpha=0.8)\n",
        "\n",
        "# Annotate each bar\n",
        "for bar in bars:\n",
        "    height = bar.get_height()\n",
        "    plt.text(bar.get_x() + bar.get_width()/2, height + 0.02,\n",
        "             f\"{height:.2f}\", ha='center', va='bottom', fontsize=10, color='black')\n",
        "\n",
        "# Add mean line\n",
        "plt.axhline(mean_val, color=\"red\", linestyle=\"--\", linewidth=2, label=f\"Mean = {mean_val:.2f}\")\n",
        "\n",
        "# Titles and labels\n",
        "plt.title(\"Semantic Similarity between Retrieved and Expected Questions\", fontsize=14, fontweight=\"bold\")\n",
        "plt.ylabel(\"Cosine Similarity\", fontsize=12)\n",
        "plt.xticks(rotation=45)\n",
        "plt.ylim(0, 1)\n",
        "plt.legend()\n",
        "plt.grid(axis='y', linestyle=\"--\", alpha=0.6)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "FFypGFeuSe_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(9,5))\n",
        "bars = plt.barh(cases, values, color=\"#90CAF9\", edgecolor=\"black\")\n",
        "plt.axvline(mean_val, color=\"red\", linestyle=\"--\", linewidth=2, label=f\"Mean = {mean_val:.2f}\")\n",
        "plt.xlabel(\"Cosine Similarity\", fontsize=12)\n",
        "plt.title(\"Semantic Similarity Scores per Test Case\", fontsize=14, fontweight=\"bold\")\n",
        "\n",
        "# Annotate bars\n",
        "for bar in bars:\n",
        "    width = bar.get_width()\n",
        "    plt.text(width + 0.02, bar.get_y() + bar.get_height()/2,\n",
        "             f\"{width:.2f}\", va='center', fontsize=10)\n",
        "\n",
        "plt.xlim(0, 1)\n",
        "plt.legend()\n",
        "plt.grid(axis='x', linestyle='--', alpha=0.6)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "G75jZclaSe8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax1 = plt.subplots(figsize=(10,5))\n",
        "ax2 = ax1.twinx()\n",
        "\n",
        "bars = ax1.bar(cases, df_cosine.avg_cosine_similarity, color=\"#4fa3d1\", alpha=0.7, label=\"Cosine Similarity\")\n",
        "line, = ax2.plot(cases, evaluation_df.precision, color=\"darkblue\", linestyle=\"--\",marker=\"o\", linewidth=2, label=\"Precision\")\n",
        "\n",
        "ax1.set_ylim(0, 1)\n",
        "ax2.set_ylim(0, 1)\n",
        "ax1.set_ylabel(\"Cosine Similarity\", fontsize=12)\n",
        "ax2.set_ylabel(\"Precision\", fontsize=12)\n",
        "plt.title(\"Comparison of Semantic Similarity and Precision per Case\", fontsize=14, fontweight=\"bold\")\n",
        "ax1.legend(handles=[bars, line], loc=\"upper right\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HiWaizw2Syn5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax1 = plt.subplots(figsize=(10,5))\n",
        "ax2 = ax1.twinx()\n",
        "\n",
        "# Bar chart for cosine similarity\n",
        "bars = ax1.bar(cases, df_cosine.avg_cosine_similarity,\n",
        "               color=\"#4fa3d1\", alpha=0.7, label=\"Cosine Similarity\")\n",
        "\n",
        "# Line plots for precision and accuracy\n",
        "line1, = ax2.plot(cases, evaluation_df.precision, color=\"darkblue\", marker=\"o\",\n",
        "                  linewidth=2, linestyle=\"--\", label=\"Precision\")\n",
        "line2, = ax2.plot(cases, evaluation_df.accuracy, color=\"darkred\", marker=\"s\",\n",
        "                  linewidth=2, linestyle=\"-.\", label=\"Accuracy\")\n",
        "\n",
        "# Axis labels and limits\n",
        "ax1.set_ylim(0, 1)\n",
        "ax2.set_ylim(0, 1)\n",
        "ax1.set_ylabel(\"Cosine Similarity\", fontsize=12)\n",
        "ax2.set_ylabel(\"Precision / Accuracy\", fontsize=12)\n",
        "\n",
        "# Titles and style\n",
        "plt.title(\"Comparison of Semantic Similarity, Precision, and Accuracy per Case\",\n",
        "          fontsize=14, fontweight=\"bold\")\n",
        "\n",
        "# Legends\n",
        "# Combine legends from both axes\n",
        "lines = [bars, line1, line2]\n",
        "labels = [l.get_label() for l in lines]\n",
        "ax1.legend(lines, labels, loc=\"upper right\")\n",
        "\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NiN0V7PoUAoV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}