{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install python-docx\n",
        "!pip install PyPDF2"
      ],
      "metadata": {
        "id": "sJpDCMPCFmeN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# upload the CSV file into the notebook\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "8N4wHYI7KgP5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import PyPDF2, docx, json, random\n",
        "from transformers import pipeline\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from pymilvus import connections, Collection\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 1Ô∏è‚É£ Connect to Zilliz Cloud\n",
        "# -----------------------------\n",
        "connections.connect(\n",
        "    alias=\"default\",\n",
        "    uri=\"https://in03-feb569ec82b1b76.serverless.aws-eu-central-1.cloud.zilliz.com\",  # your URI\n",
        "    token=\"520e0e883ef97fcc4663dca8514090a2a491dd29b714711e961807fdbff8a163d82abd308cf1a8ef546698eb1759aa7054cbc295\")\n",
        "\n",
        "\n",
        "collection = Collection(\"interveiw_Knowledge\")\n",
        "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n"
      ],
      "metadata": {
        "id": "Im0ib-nvRF5z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# 2Ô∏è‚É£ Load LLM (small one for demo)\n",
        "# -----------------------------\n",
        "\n",
        "\n",
        "# Load LLM (small one for demo)\n",
        "chatbot_model = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=\"HuggingFaceH4/zephyr-7b-beta\",\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=\"auto\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "uIirWN47np5d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IQC1clNxnB2H"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "import PyPDF2, docx, json, random\n",
        "from transformers import pipeline\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from pymilvus import connections, Collection\n",
        "import random\n",
        "\n",
        "# Store sessions for mock interview\n",
        "interview_sessions = {}\n",
        "\n",
        "# -----------------------------\n",
        "# 3Ô∏è‚É£ File Extractors\n",
        "# -----------------------------\n",
        "def extract_text_from_pdf(file_path):\n",
        "    text = \"\"\n",
        "    with open(file_path, \"rb\") as f:\n",
        "        reader = PyPDF2.PdfReader(f)\n",
        "        for page in reader.pages:\n",
        "            page_text = page.extract_text()\n",
        "            if page_text:\n",
        "                text += page_text + \"\\n\"\n",
        "    return text.strip()\n",
        "\n",
        "\n",
        "def extract_text_from_docx(file_path):\n",
        "    doc = docx.Document(file_path)\n",
        "    return \"\\n\".join([p.text for p in doc.paragraphs if p.text.strip()])\n",
        "\n",
        "\n",
        "def extract_text(file_path):\n",
        "    if file_path.endswith(\".pdf\"):\n",
        "        return extract_text_from_pdf(file_path)\n",
        "    elif file_path.endswith(\".docx\"):\n",
        "        return extract_text_from_docx(file_path)\n",
        "    return \"\"\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 4Ô∏è‚É£ RAG Retrieval Function\n",
        "# -----------------------------\n",
        "def retrieve_questions_from_zilliz(cv_text, jd_text, top_k=5):\n",
        "    # If user uploads blank or short files\n",
        "    if not cv_text.strip() or not jd_text.strip():\n",
        "        return get_random_questions_from_collection(top_k)\n",
        "\n",
        "    # Create a meaningful semantic search query\n",
        "    query = f\"{jd_text[:500]} related interview questions for candidate with {cv_text[:500]}\"\n",
        "    query_vector = embedder.encode([query]).tolist()\n",
        "\n",
        "    search_params = {\"metric_type\": \"COSINE\", \"params\": {\"nprobe\": 10}}\n",
        "\n",
        "    results = collection.search(\n",
        "        data=query_vector,\n",
        "        anns_field=\"embedding\",\n",
        "        param=search_params,\n",
        "        limit=50,\n",
        "        output_fields=[\"text\"]\n",
        "    )\n",
        "\n",
        "    # Extract retrieved question texts\n",
        "    hits = [hit.entity.get(\"text\") for hit in results[0] if hit.distance > 0]\n",
        "\n",
        "    # Remove duplicates while preserving order\n",
        "    hits = list(dict.fromkeys(hits))\n",
        "\n",
        "    # Fallback if no results found\n",
        "    if not hits:\n",
        "        hits = get_random_questions_from_collection(top_k)\n",
        "\n",
        "    # Randomize subset\n",
        "    hits = random.sample(hits, min(top_k, len(hits)))\n",
        "    return hits\n",
        "\n",
        "\n",
        "def get_random_questions_from_collection(n=5):\n",
        "    \"\"\"Fetch random questions from the whole collection.\"\"\"\n",
        "    data = collection.query(expr=\"\", output_fields=[\"text\"], limit=100)\n",
        "    all_qs = [d[\"text\"] for d in data]\n",
        "    return random.sample(all_qs, min(n, len(all_qs)))\n",
        "\n",
        "# -----------------------------\n",
        "# 5Ô∏è‚É£ Generate Summary + Questions\n",
        "# -----------------------------\n",
        "def generate_summary_and_questions(cv_file, jd_file):\n",
        "    if not cv_file or not jd_file:\n",
        "        return \"‚ö†Ô∏è Please upload both a CV and Job Description.\", []\n",
        "\n",
        "    cv_text = extract_text(cv_file)\n",
        "    jd_text = extract_text(jd_file)\n",
        "\n",
        "    questions = retrieve_questions_from_zilliz(cv_text, jd_text, top_k=5)\n",
        "\n",
        "    if not questions:\n",
        "        return \"No relevant questions found. Try a different job description.\", []\n",
        "\n",
        "    summary = (\n",
        "        f\"üéØ **Top {len(questions)} relevant questions retrieved from knowledge base:**\\n\\n\"\n",
        "        + \"\\n\".join([f\"{i+1}. {q}\" for i, q in enumerate(questions)])\n",
        "    )\n",
        "    return summary, questions\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 6Ô∏è‚É£ Feedback Generator\n",
        "# -----------------------------\n",
        "def give_feedback(answer, question):\n",
        "    prompt = (\n",
        "        f\"Evaluate the following candidate answer to a technical question.\\n\\n\"\n",
        "        f\"Question: {question}\\n\"\n",
        "        f\"Candidate's Answer: {answer}\\n\\n\"\n",
        "        f\"Your response must include only:\\n\"\n",
        "        f\"1Ô∏è‚É£ Evaluation: State 'Correct', 'Partially Correct', or 'Incorrect'.\\n\"\n",
        "        f\"2Ô∏è‚É£ Correct Answer: Write the ideal answer in 1‚Äì2 short sentences.\\n\"\n",
        "        f\"3Ô∏è‚É£ Review:\\n\"\n",
        "        f\"   - ‚úÖ Strengths (1 short sentence)\\n\"\n",
        "        f\"   - ‚ö†Ô∏è Areas for improvement (1 short sentence)\\n\\n\"\n",
        "        f\"Keep your total response under 100 words. Do NOT show any examples or repeat the question.\"\n",
        "    )\n",
        "\n",
        "    response = chatbot_model(\n",
        "        prompt,\n",
        "        max_new_tokens=150,\n",
        "        do_sample=False,\n",
        "        temperature=0.3,\n",
        "        top_p=0.9\n",
        "    )[0][\"generated_text\"]\n",
        "\n",
        "    return response.replace(prompt, \"\").strip()\n",
        "\n",
        "\n",
        "\n",
        "## -----------------------------\n",
        "# 7Ô∏è‚É£ Mock Interview Logic (Final Fixed Version)\n",
        "# -----------------------------\n",
        "def start_interview(questions):\n",
        "    \"\"\"Start the interview and initialize session.\"\"\"\n",
        "    if not questions:\n",
        "        return \"‚ö†Ô∏è No questions available. Please upload CV + JD first.\", {}\n",
        "\n",
        "    session_data = {\"questions\": questions, \"current\": 0}\n",
        "    first_q = questions[0]\n",
        "    status_msg = f\"üé§ **Starting personalized mock interview!**\\n\\nüß† **First question:**\\n{first_q}\"\n",
        "    return status_msg, session_data\n",
        "\n",
        "\n",
        "def chat_with_bot(message, history, session_data):\n",
        "    \"\"\"Handle user responses and provide feedback.\"\"\"\n",
        "    if not session_data or \"questions\" not in session_data or not session_data[\"questions\"]:\n",
        "        return \"‚ö†Ô∏è Please start the mock interview first.\", session_data\n",
        "\n",
        "    q_index = session_data.get(\"current\", 0)\n",
        "    questions = session_data[\"questions\"]\n",
        "\n",
        "    current_question = questions[q_index]\n",
        "    feedback = give_feedback(message, current_question)\n",
        "\n",
        "    q_index += 1\n",
        "    if q_index < len(questions):\n",
        "        session_data[\"current\"] = q_index\n",
        "        next_q = questions[q_index]\n",
        "        progress = f\"({q_index}/{len(questions)})\"\n",
        "        reply = f\"üí¨ **Feedback:** {feedback}\\n\\n‚û°Ô∏è **Next question {progress}:** {next_q}\"\n",
        "    else:\n",
        "        reply = f\"üí¨ **Final Feedback:** {feedback}\\n\\n‚úÖ Interview finished. Great job!\"\n",
        "        session_data = {}  # reset after interview\n",
        "\n",
        "    return reply, session_data\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 8Ô∏è‚É£ Gradio Interface\n",
        "# -----------------------------\n",
        "with gr.Blocks(theme=gr.themes.Soft(), title=\"RAG Mock Interview Coach\") as demo:\n",
        "    gr.Markdown(\"## ü§ñ CV + JD Powered Mock Interview Coach (Now with RAG & Zilliz Cloud)\")\n",
        "\n",
        "    with gr.Tabs():\n",
        "        # Tab 1: Upload & Analyze\n",
        "        with gr.Tab(\"Upload & Analyze\"):\n",
        "            gr.Markdown(\"### üìÑ Upload your CV and Job Description to generate relevant questions.\")\n",
        "            with gr.Row():\n",
        "                cv_upload = gr.File(label=\"Upload CV (PDF or DOCX)\", type=\"filepath\")\n",
        "                jd_upload = gr.File(label=\"Upload Job Description (PDF or DOCX)\", type=\"filepath\")\n",
        "\n",
        "            output = gr.Markdown(label=\"Extracted Info & Questions\")\n",
        "            role_questions = gr.State([])\n",
        "\n",
        "            generate_btn = gr.Button(\"üîç Analyze CV + JD & Retrieve Questions\")\n",
        "            generate_btn.click(\n",
        "                fn=generate_summary_and_questions,\n",
        "                inputs=[cv_upload, jd_upload],\n",
        "                outputs=[output, role_questions]\n",
        "            )\n",
        "\n",
        "        # Tab 2: Mock Interview\n",
        "        with gr.Tab(\"Mock Interview\"):\n",
        "            gr.Markdown(\"### üé§ Start Your Personalized Mock Interview\")\n",
        "\n",
        "            # Session state to persist across chat\n",
        "            session_state = gr.State({})\n",
        "\n",
        "            start_btn = gr.Button(\"üöÄ Start Mock Interview\")\n",
        "            start_output = gr.Markdown(label=\"Interview Status\")\n",
        "\n",
        "            # Start interview properly ‚Äî split text + state\n",
        "            def start_and_show(questions):\n",
        "                message, session = start_interview(questions)\n",
        "                return message, session\n",
        "\n",
        "            start_btn.click(\n",
        "                fn=start_and_show,\n",
        "                inputs=[role_questions],\n",
        "                outputs=[start_output, session_state]\n",
        "            )\n",
        "\n",
        "            chatbot = gr.ChatInterface(\n",
        "                fn=chat_with_bot,\n",
        "                title=\"Interview Coach\",\n",
        "                description=\"üí¨ Answer the question, and get instant feedback!\",\n",
        "                additional_inputs=[session_state],\n",
        "                additional_outputs=[session_state]\n",
        "            )\n",
        "\n",
        "demo.launch()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building RAG"
      ],
      "metadata": {
        "id": "ugbeurCvaLqj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymilvus\n"
      ],
      "metadata": {
        "id": "IyCSg1gZaHnk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"in03-feb569ec82b1b76.serverless.aws-eu-central-1.cloud.zilliz.com\"\n",
        "token = \"520e0e883ef97fcc4663dca8514090a2a491dd29b714711e961807fdbff8a163d82abd308cf1a8ef546698eb1759aa7054cbc295\""
      ],
      "metadata": {
        "id": "Lf3VTu1a9arJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pymilvus import connections\n",
        "\n",
        "connections.connect(\n",
        "    alias=\"default\",\n",
        "    uri=\"https://in03-feb569ec82b1b76.serverless.aws-eu-central-1.cloud.zilliz.com\",\n",
        "    token=\"520e0e883ef97fcc4663dca8514090a2a491dd29b714711e961807fdbff8a163d82abd308cf1a8ef546698eb1759aa7054cbc295\"\n",
        ")"
      ],
      "metadata": {
        "id": "fqPRBXLZ-F2B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pymilvus import Collection, connections\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import json\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 2Ô∏è‚É£ Load Collection\n",
        "# -----------------------------\n",
        "collection = Collection(\"interveiw_Knowledge\")\n",
        "\n",
        "# -----------------------------\n",
        "# 3Ô∏è‚É£ Load JSON Data\n",
        "# -----------------------------\n",
        "with open(\"interview_data.json\", \"r\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# -----------------------------\n",
        "# 4Ô∏è‚É£ Create Texts + Embeddings\n",
        "# -----------------------------\n",
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "texts = [f\"{item['role']} - {item['skill']} - {item['question']}\" for item in data]\n",
        "embeddings = model.encode(texts).tolist()\n",
        "\n",
        "print(f\"Embedding dimension: {len(embeddings[0])}\")\n",
        "\n",
        "# -----------------------------\n",
        "# 5Ô∏è‚É£ Insert (ORDER MATTERS!)\n",
        "# -----------------------------\n",
        "entities = [texts, embeddings]  # text first, then embedding\n",
        "insert_result = collection.insert(entities)\n",
        "collection.flush()\n",
        "\n",
        "print(f\"‚úÖ Inserted {len(texts)} records successfully!\")\n",
        "print(\"Inserted IDs:\", insert_result.primary_keys)\n",
        "\n",
        "# -----------------------------\n",
        "# 6Ô∏è‚É£ Verify entity count\n",
        "# -----------------------------\n",
        "print(\"Total entities in collection:\", collection.num_entities)\n"
      ],
      "metadata": {
        "id": "hAIBqIdg_yF8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Explain the difference between supervised and unsupervised learning.\"\n",
        "query_vector = model.encode([query]).tolist()\n",
        "\n",
        "search_params = {\"metric_type\": \"COSINE\", \"params\": {\"nprobe\": 10}}\n",
        "results = collection.search(\n",
        "    data=query_vector,\n",
        "    anns_field=\"embedding\",\n",
        "    param=search_params,\n",
        "    limit=3,\n",
        "    output_fields=[\"text\"]\n",
        ")\n",
        "\n",
        "print(\"\\nüîç Top matches:\")\n",
        "for hit in results[0]:\n",
        "    print(f\"Score: {hit.distance:.4f} | Text: {hit.entity.get('text')}\")\n"
      ],
      "metadata": {
        "id": "AIdh7C-HPosv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from pymilvus import Collection, connections\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# -----------------------------\n",
        "# 1Ô∏è‚É£ Connect to Zilliz Cloud\n",
        "# -----------------------------\n",
        "connections.connect(\n",
        "    alias=\"default\",\n",
        "    uri=\"https://in03-feb569ec82b1b76.serverless.aws-eu-central-1.cloud.zilliz.com\",\n",
        "    token=\"520e0e883ef97fcc4663dca8514090a2a491dd29b714711e961807fdbff8a163d82abd308cf1a8ef546698eb1759aa7054cbc295\"\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# 2Ô∏è‚É£ Load Existing Collection\n",
        "# -----------------------------\n",
        "collection = Collection(\"interveiw_Knowledge\")\n",
        "\n",
        "# -----------------------------\n",
        "# 3Ô∏è‚É£ Load CSV Data\n",
        "# -----------------------------\n",
        "df = pd.read_csv(\"interview_dataset_enhanced.csv\")\n",
        "\n",
        "print(f\"Loaded {len(df)} rows from CSV\")\n",
        "\n",
        "# -----------------------------\n",
        "# 4Ô∏è‚É£ Create Combined Texts\n",
        "# -----------------------------\n",
        "texts = [f\"{row.role} - {row.skill} - {row.question}\" for _, row in df.iterrows()]\n",
        "\n",
        "# -----------------------------\n",
        "# 5Ô∏è‚É£ Create Embeddings\n",
        "# -----------------------------\n",
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "embeddings = model.encode(texts).tolist()\n",
        "\n",
        "print(f\"‚úÖ Embedding dimension: {len(embeddings[0])}\")\n",
        "\n",
        "# -----------------------------\n",
        "# 6Ô∏è‚É£ Insert into Collection\n",
        "# -----------------------------\n",
        "entities = [texts, embeddings]  # Must match your schema order\n",
        "insert_result = collection.insert(entities)\n",
        "collection.flush()\n",
        "\n",
        "print(f\"‚úÖ Inserted {len(texts)} records successfully!\")\n",
        "print(\"Inserted IDs:\", insert_result.primary_keys)\n",
        "\n",
        "# -----------------------------\n",
        "# 7Ô∏è‚É£ Verify Entity Count\n",
        "# -----------------------------\n",
        "print(\"Total entities in collection:\", collection.num_entities)\n"
      ],
      "metadata": {
        "id": "2as2vZFX9P1k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pymilvus import Collection, connections\n",
        "\n",
        "connections.connect(\n",
        "    alias=\"default\",\n",
        "    uri=\"https://in03-feb569ec82b1b76.serverless.aws-eu-central-1.cloud.zilliz.com\",\n",
        "    token=\"520e0e883ef97fcc4663dca8514090a2a491dd29b714711e961807fdbff8a163d82abd308cf1a8ef546698eb1759aa7054cbc295\"\n",
        ")\n",
        "\n",
        "collection = Collection(\"interveiw_Knowledge\")\n",
        "collection.load()\n",
        "\n",
        "print(collection.schema)  # üëà check field names + dimensions\n",
        "\n",
        "# Preview 5 entries\n",
        "results = collection.query(expr=\"\", output_fields=[\"text\"], limit=5)\n",
        "for r in results:\n",
        "    print(r[\"text\"])\n"
      ],
      "metadata": {
        "id": "Cuhjpt9JCT87"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}