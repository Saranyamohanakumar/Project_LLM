{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install python-docx\n",
        "!pip install PyPDF2"
      ],
      "metadata": {
        "id": "sJpDCMPCFmeN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import PyPDF2, docx, json, random\n",
        "from transformers import pipeline\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from pymilvus import connections, Collection\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 1Ô∏è‚É£ Connect to Zilliz Cloud\n",
        "# -----------------------------\n",
        "connections.connect(\n",
        "    alias=\"default\",\n",
        "    uri=\"https://in03-feb569ec82b1b76.serverless.aws-eu-central-1.cloud.zilliz.com\",  # your URI\n",
        "    token=\"520e0e883ef97fcc4663dca8514090a2a491dd29b714711e961807fdbff8a163d82abd308cf1a8ef546698eb1759aa7054cbc295\")\n",
        "\n",
        "\n",
        "collection = Collection(\"interveiw_Knowledge\")\n",
        "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n"
      ],
      "metadata": {
        "id": "Im0ib-nvRF5z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# 2Ô∏è‚É£ Load LLM (small one for demo)\n",
        "# -----------------------------\n",
        "\n",
        "\n",
        "# Load LLM (small one for demo)\n",
        "chatbot_model = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=\"HuggingFaceH4/zephyr-7b-beta\",\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=\"auto\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "uIirWN47np5d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IQC1clNxnB2H"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "import PyPDF2, docx, json, random\n",
        "from transformers import pipeline\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from pymilvus import connections, Collection\n",
        "import random\n",
        "\n",
        "# Store sessions for mock interview\n",
        "interview_sessions = {}\n",
        "\n",
        "# -----------------------------\n",
        "# 3Ô∏è‚É£ File Extractors\n",
        "# -----------------------------\n",
        "def extract_text_from_pdf(file_path):\n",
        "    text = \"\"\n",
        "    with open(file_path, \"rb\") as f:\n",
        "        reader = PyPDF2.PdfReader(f)\n",
        "        for page in reader.pages:\n",
        "            page_text = page.extract_text()\n",
        "            if page_text:\n",
        "                text += page_text + \"\\n\"\n",
        "    return text.strip()\n",
        "\n",
        "\n",
        "def extract_text_from_docx(file_path):\n",
        "    doc = docx.Document(file_path)\n",
        "    return \"\\n\".join([p.text for p in doc.paragraphs if p.text.strip()])\n",
        "\n",
        "\n",
        "def extract_text(file_path):\n",
        "    if file_path.endswith(\".pdf\"):\n",
        "        return extract_text_from_pdf(file_path)\n",
        "    elif file_path.endswith(\".docx\"):\n",
        "        return extract_text_from_docx(file_path)\n",
        "    return \"\"\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 4Ô∏è‚É£ RAG Retrieval Function\n",
        "# -----------------------------\n",
        "def retrieve_questions_from_zilliz(cv_text, jd_text, top_k=5):\n",
        "    # If user uploads blank or short files\n",
        "    if not cv_text.strip() or not jd_text.strip():\n",
        "        return get_random_questions_from_collection(top_k)\n",
        "\n",
        "    # Create a meaningful semantic search query\n",
        "    query = f\"{jd_text[:500]} related interview questions for candidate with {cv_text[:500]}\"\n",
        "    query_vector = embedder.encode([query]).tolist()\n",
        "\n",
        "    search_params = {\"metric_type\": \"COSINE\", \"params\": {\"nprobe\": 10}}\n",
        "\n",
        "    results = collection.search(\n",
        "        data=query_vector,\n",
        "        anns_field=\"embedding\",\n",
        "        param=search_params,\n",
        "        limit=50,\n",
        "        output_fields=[\"text\"]\n",
        "    )\n",
        "\n",
        "    # Extract retrieved question texts\n",
        "    hits = [hit.entity.get(\"text\") for hit in results[0] if hit.distance > 0]\n",
        "\n",
        "    # Remove duplicates while preserving order\n",
        "    hits = list(dict.fromkeys(hits))\n",
        "\n",
        "    # Fallback if no results found\n",
        "    if not hits:\n",
        "        hits = get_random_questions_from_collection(top_k)\n",
        "\n",
        "    # Randomize subset\n",
        "    hits = random.sample(hits, min(top_k, len(hits)))\n",
        "    return hits\n",
        "\n",
        "\n",
        "def get_random_questions_from_collection(n=5):\n",
        "    \"\"\"Fetch random questions from the whole collection.\"\"\"\n",
        "    data = collection.query(expr=\"\", output_fields=[\"text\"], limit=100)\n",
        "    all_qs = [d[\"text\"] for d in data]\n",
        "    return random.sample(all_qs, min(n, len(all_qs)))\n",
        "\n",
        "# -----------------------------\n",
        "# 5Ô∏è‚É£ Generate Summary + Questions\n",
        "# -----------------------------\n",
        "def generate_summary_and_questions(cv_file, jd_file):\n",
        "    if not cv_file or not jd_file:\n",
        "        return \"‚ö†Ô∏è Please upload both a CV and Job Description.\", []\n",
        "\n",
        "    cv_text = extract_text(cv_file)\n",
        "    jd_text = extract_text(jd_file)\n",
        "\n",
        "    questions = retrieve_questions_from_zilliz(cv_text, jd_text, top_k=5)\n",
        "\n",
        "    if not questions:\n",
        "        return \"No relevant questions found. Try a different job description.\", []\n",
        "\n",
        "    summary = (\n",
        "        f\"üéØ **Top {len(questions)} relevant questions retrieved from knowledge base:**\\n\\n\"\n",
        "        + \"\\n\".join([f\"{i+1}. {q}\" for i, q in enumerate(questions)])\n",
        "    )\n",
        "    return summary, questions\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 6Ô∏è‚É£ Feedback Generator\n",
        "# -----------------------------\n",
        "def give_feedback(answer, question):\n",
        "    prompt = (\n",
        "        f\"Evaluate the following candidate answer to a technical question.\\n\\n\"\n",
        "        f\"Question: {question}\\n\"\n",
        "        f\"Candidate's Answer: {answer}\\n\\n\"\n",
        "        f\"Your response must include only:\\n\"\n",
        "        f\"1Ô∏è‚É£ Evaluation: State 'Correct', 'Partially Correct', or 'Incorrect'.\\n\"\n",
        "        f\"2Ô∏è‚É£ Correct Answer: Write the ideal answer in 1‚Äì2 short sentences.\\n\"\n",
        "        f\"3Ô∏è‚É£ Review:\\n\"\n",
        "        f\"   - ‚úÖ Strengths (1 short sentence)\\n\"\n",
        "        f\"   - ‚ö†Ô∏è Areas for improvement (1 short sentence)\\n\\n\"\n",
        "        f\"Keep your total response under 100 words. Do NOT show any examples or repeat the question.\"\n",
        "    )\n",
        "\n",
        "    response = chatbot_model(\n",
        "        prompt,\n",
        "        max_new_tokens=150,\n",
        "        do_sample=False,\n",
        "        temperature=0.3,\n",
        "        top_p=0.9\n",
        "    )[0][\"generated_text\"]\n",
        "\n",
        "    return response.replace(prompt, \"\").strip()\n",
        "\n",
        "\n",
        "\n",
        "## -----------------------------\n",
        "# 7Ô∏è‚É£ Mock Interview Logic (Final Fixed Version)\n",
        "# -----------------------------\n",
        "def start_interview(questions):\n",
        "    \"\"\"Start the interview and initialize session.\"\"\"\n",
        "    if not questions:\n",
        "        return \"‚ö†Ô∏è No questions available. Please upload CV + JD first.\", {}\n",
        "\n",
        "    session_data = {\"questions\": questions, \"current\": 0}\n",
        "    first_q = questions[0]\n",
        "    status_msg = f\"üé§ **Starting personalized mock interview!**\\n\\nüß† **First question:**\\n{first_q}\"\n",
        "    return status_msg, session_data\n",
        "\n",
        "\n",
        "def chat_with_bot(message, history, session_data):\n",
        "    \"\"\"Handle user responses and provide feedback.\"\"\"\n",
        "    if not session_data or \"questions\" not in session_data or not session_data[\"questions\"]:\n",
        "        return \"‚ö†Ô∏è Please start the mock interview first.\", session_data\n",
        "\n",
        "    q_index = session_data.get(\"current\", 0)\n",
        "    questions = session_data[\"questions\"]\n",
        "\n",
        "    current_question = questions[q_index]\n",
        "    feedback = give_feedback(message, current_question)\n",
        "\n",
        "    q_index += 1\n",
        "    if q_index < len(questions):\n",
        "        session_data[\"current\"] = q_index\n",
        "        next_q = questions[q_index]\n",
        "        progress = f\"({q_index}/{len(questions)})\"\n",
        "        reply = f\"üí¨ **Feedback:** {feedback}\\n\\n‚û°Ô∏è **Next question {progress}:** {next_q}\"\n",
        "    else:\n",
        "        reply = f\"üí¨ **Final Feedback:** {feedback}\\n\\n‚úÖ Interview finished. Great job!\"\n",
        "        session_data = {}  # reset after interview\n",
        "\n",
        "    return reply, session_data\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 8Ô∏è‚É£ Gradio Interface\n",
        "# -----------------------------\n",
        "with gr.Blocks(theme=gr.themes.Soft(), title=\"RAG Mock Interview Coach\") as demo:\n",
        "    gr.Markdown(\"## ü§ñ CV + JD Powered Mock Interview Coach (Now with RAG & Zilliz Cloud)\")\n",
        "\n",
        "    with gr.Tabs():\n",
        "        # Tab 1: Upload & Analyze\n",
        "        with gr.Tab(\"Upload & Analyze\"):\n",
        "            gr.Markdown(\"### üìÑ Upload your CV and Job Description to generate relevant questions.\")\n",
        "            with gr.Row():\n",
        "                cv_upload = gr.File(label=\"Upload CV (PDF or DOCX)\", type=\"filepath\")\n",
        "                jd_upload = gr.File(label=\"Upload Job Description (PDF or DOCX)\", type=\"filepath\")\n",
        "\n",
        "            output = gr.Markdown(label=\"Extracted Info & Questions\")\n",
        "            role_questions = gr.State([])\n",
        "\n",
        "            generate_btn = gr.Button(\"üîç Analyze CV + JD & Retrieve Questions\")\n",
        "            generate_btn.click(\n",
        "                fn=generate_summary_and_questions,\n",
        "                inputs=[cv_upload, jd_upload],\n",
        "                outputs=[output, role_questions]\n",
        "            )\n",
        "\n",
        "        # Tab 2: Mock Interview\n",
        "        with gr.Tab(\"Mock Interview\"):\n",
        "            gr.Markdown(\"### üé§ Start Your Personalized Mock Interview\")\n",
        "\n",
        "            # Session state to persist across chat\n",
        "            session_state = gr.State({})\n",
        "\n",
        "            start_btn = gr.Button(\"üöÄ Start Mock Interview\")\n",
        "            start_output = gr.Markdown(label=\"Interview Status\")\n",
        "\n",
        "            # Start interview properly ‚Äî split text + state\n",
        "            def start_and_show(questions):\n",
        "                message, session = start_interview(questions)\n",
        "                return message, session\n",
        "\n",
        "            start_btn.click(\n",
        "                fn=start_and_show,\n",
        "                inputs=[role_questions],\n",
        "                outputs=[start_output, session_state]\n",
        "            )\n",
        "\n",
        "            chatbot = gr.ChatInterface(\n",
        "                fn=chat_with_bot,\n",
        "                title=\"Interview Coach\",\n",
        "                description=\"üí¨ Answer the question, and get instant feedback!\",\n",
        "                additional_inputs=[session_state],\n",
        "                additional_outputs=[session_state]\n",
        "            )\n",
        "\n",
        "demo.launch()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building RAG"
      ],
      "metadata": {
        "id": "ugbeurCvaLqj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymilvus\n"
      ],
      "metadata": {
        "id": "IyCSg1gZaHnk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"in03-feb569ec82b1b76.serverless.aws-eu-central-1.cloud.zilliz.com\"\n",
        "token = \"520e0e883ef97fcc4663dca8514090a2a491dd29b714711e961807fdbff8a163d82abd308cf1a8ef546698eb1759aa7054cbc295\""
      ],
      "metadata": {
        "id": "Lf3VTu1a9arJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pymilvus import connections\n",
        "\n",
        "connections.connect(\n",
        "    alias=\"default\",\n",
        "    uri=\"https://in03-feb569ec82b1b76.serverless.aws-eu-central-1.cloud.zilliz.com\",\n",
        "    token=\"520e0e883ef97fcc4663dca8514090a2a491dd29b714711e961807fdbff8a163d82abd308cf1a8ef546698eb1759aa7054cbc295\"\n",
        ")"
      ],
      "metadata": {
        "id": "fqPRBXLZ-F2B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# upload the CSV file into the notebook\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "8N4wHYI7KgP5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pymilvus import Collection, connections\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import json\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 2Ô∏è‚É£ Load Collection\n",
        "# -----------------------------\n",
        "collection = Collection(\"interveiw_Knowledge\")\n",
        "\n",
        "# -----------------------------\n",
        "# 3Ô∏è‚É£ Load JSON Data\n",
        "# -----------------------------\n",
        "with open(\"interview_data.json\", \"r\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# -----------------------------\n",
        "# 4Ô∏è‚É£ Create Texts + Embeddings\n",
        "# -----------------------------\n",
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "texts = [f\"{item['role']} - {item['skill']} - {item['question']}\" for item in data]\n",
        "embeddings = model.encode(texts).tolist()\n",
        "\n",
        "print(f\"Embedding dimension: {len(embeddings[0])}\")\n",
        "\n",
        "# -----------------------------\n",
        "# 5Ô∏è‚É£ Insert (ORDER MATTERS!)\n",
        "# -----------------------------\n",
        "entities = [texts, embeddings]  # text first, then embedding\n",
        "insert_result = collection.insert(entities)\n",
        "collection.flush()\n",
        "\n",
        "print(f\"‚úÖ Inserted {len(texts)} records successfully!\")\n",
        "print(\"Inserted IDs:\", insert_result.primary_keys)\n",
        "\n",
        "# -----------------------------\n",
        "# 6Ô∏è‚É£ Verify entity count\n",
        "# -----------------------------\n",
        "print(\"Total entities in collection:\", collection.num_entities)\n"
      ],
      "metadata": {
        "id": "hAIBqIdg_yF8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Explain the difference between supervised and unsupervised learning.\"\n",
        "query_vector = model.encode([query]).tolist()\n",
        "\n",
        "search_params = {\"metric_type\": \"COSINE\", \"params\": {\"nprobe\": 10}}\n",
        "results = collection.search(\n",
        "    data=query_vector,\n",
        "    anns_field=\"embedding\",\n",
        "    param=search_params,\n",
        "    limit=3,\n",
        "    output_fields=[\"text\"]\n",
        ")\n",
        "\n",
        "print(\"\\nüîç Top matches:\")\n",
        "for hit in results[0]:\n",
        "    print(f\"Score: {hit.distance:.4f} | Text: {hit.entity.get('text')}\")\n"
      ],
      "metadata": {
        "id": "AIdh7C-HPosv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from pymilvus import Collection, connections\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# -----------------------------\n",
        "# 1Ô∏è‚É£ Connect to Zilliz Cloud\n",
        "# -----------------------------\n",
        "connections.connect(\n",
        "    alias=\"default\",\n",
        "    uri=\"https://in03-feb569ec82b1b76.serverless.aws-eu-central-1.cloud.zilliz.com\",\n",
        "    token=\"520e0e883ef97fcc4663dca8514090a2a491dd29b714711e961807fdbff8a163d82abd308cf1a8ef546698eb1759aa7054cbc295\"\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# 2Ô∏è‚É£ Load Existing Collection\n",
        "# -----------------------------\n",
        "collection = Collection(\"interveiw_Knowledge\")\n",
        "\n",
        "# -----------------------------\n",
        "# 3Ô∏è‚É£ Load CSV Data\n",
        "# -----------------------------\n",
        "df = pd.read_csv(\"interview_dataset_enhanced.csv\")\n",
        "\n",
        "print(f\"Loaded {len(df)} rows from CSV\")\n",
        "\n",
        "# -----------------------------\n",
        "# 4Ô∏è‚É£ Create Combined Texts\n",
        "# -----------------------------\n",
        "texts = [f\"{row.role} - {row.skill} - {row.question}\" for _, row in df.iterrows()]\n",
        "\n",
        "# -----------------------------\n",
        "# 5Ô∏è‚É£ Create Embeddings\n",
        "# -----------------------------\n",
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "embeddings = model.encode(texts).tolist()\n",
        "\n",
        "print(f\"‚úÖ Embedding dimension: {len(embeddings[0])}\")\n",
        "\n",
        "# -----------------------------\n",
        "# 6Ô∏è‚É£ Insert into Collection\n",
        "# -----------------------------\n",
        "entities = [texts, embeddings]  # Must match your schema order\n",
        "insert_result = collection.insert(entities)\n",
        "collection.flush()\n",
        "\n",
        "print(f\"‚úÖ Inserted {len(texts)} records successfully!\")\n",
        "print(\"Inserted IDs:\", insert_result.primary_keys)\n",
        "\n",
        "# -----------------------------\n",
        "# 7Ô∏è‚É£ Verify Entity Count\n",
        "# -----------------------------\n",
        "print(\"Total entities in collection:\", collection.num_entities)\n"
      ],
      "metadata": {
        "id": "2as2vZFX9P1k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pymilvus import Collection, connections\n",
        "\n",
        "connections.connect(\n",
        "    alias=\"default\",\n",
        "    uri=\"https://in03-feb569ec82b1b76.serverless.aws-eu-central-1.cloud.zilliz.com\",\n",
        "    token=\"520e0e883ef97fcc4663dca8514090a2a491dd29b714711e961807fdbff8a163d82abd308cf1a8ef546698eb1759aa7054cbc295\"\n",
        ")\n",
        "\n",
        "collection = Collection(\"interveiw_Knowledge\")\n",
        "collection.load()\n",
        "\n",
        "print(collection.schema)  # üëà check field names + dimensions\n",
        "\n",
        "# Preview 5 entries\n",
        "results = collection.query(expr=\"\", output_fields=[\"text\"], limit=5)\n",
        "for r in results:\n",
        "    print(r[\"text\"])\n"
      ],
      "metadata": {
        "id": "Cuhjpt9JCT87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Retrieval Evaluation (Accuracy, Precision, Recall)bold text"
      ],
      "metadata": {
        "id": "guewajyM_0Xh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_cases = [\n",
        "    {\n",
        "        \"cv\": \"Experienced in SQL and data integration using ETL tools.\",\n",
        "        \"jd\": \"Looking for a Data Engineer with expertise in SQL joins and ETL pipelines.\",\n",
        "        \"expected\": [\n",
        "            \"Data Engineer - SQL - Can you explain the difference between INNER JOIN and LEFT JOIN in SQL?\",\n",
        "            \"Data Engineer - ETL - What is the purpose of an ETL pipeline?\",\n",
        "            \"Data Engineer - ETL - How do you handle duplicate data in ETL pipelines?\"\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"cv\": \"Skilled in Python and data cleaning with pandas.\",\n",
        "        \"jd\": \"Hiring a Data Engineer who can handle missing values and large datasets in Python.\",\n",
        "        \"expected\": [\n",
        "            \"Data Engineer - Python - How would you handle missing values in a large dataset using Python?\",\n",
        "            \"Data Engineer - Python - How do you read a large CSV file efficiently in Python?\",\n",
        "            \"Data Engineer - Python - How do you optimize Pandas operations for large datasets?\"\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"cv\": \"Worked on Hadoop and Spark for distributed big data processing.\",\n",
        "        \"jd\": \"Seeking a Data Engineer skilled in Hadoop, Spark, and distributed computing.\",\n",
        "        \"expected\": [\n",
        "            \"Data Engineer - Big Data - Can you explain the difference between Hadoop and Spark?\",\n",
        "            \"Data Engineer - Big Data - What are the main components of Hadoop?\",\n",
        "            \"Data Engineer - Big Data - What is partitioning in Spark?\"\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"cv\": \"Experienced with cloud services and data warehousing.\",\n",
        "        \"jd\": \"Looking for a Data Engineer experienced with AWS and Redshift.\",\n",
        "        \"expected\": [\n",
        "            \"Data Engineer - Cloud - How do you set up data pipelines in AWS?\",\n",
        "            \"Data Engineer - Cloud - What is the difference between AWS Redshift and Snowflake?\",\n",
        "            \"Data Engineer - Cloud - What is autoscaling in cloud services?\"\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"cv\": \"Worked on model deployment pipelines and automation.\",\n",
        "        \"jd\": \"Hiring an ML Engineer with MLOps and CI/CD experience.\",\n",
        "        \"expected\": [\n",
        "            \"ML Engineer - MLOps - What is CI/CD in the context of ML?\",\n",
        "            \"ML Engineer - Deployment - What is containerization and why is it useful for ML models?\",\n",
        "            \"ML Engineer - MLOps - What is a feature store in MLOps?\"\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"cv\": \"Strong knowledge of regression and machine learning evaluation.\",\n",
        "        \"jd\": \"Looking for a Data Scientist experienced in model metrics and overfitting.\",\n",
        "        \"expected\": [\n",
        "            \"Data Scientist - Model Evaluation - What metrics would you use for a regression model?\",\n",
        "            \"Data Scientist - Machine Learning - What is overfitting in machine learning?\",\n",
        "            \"Data Scientist - Machine Learning - What is the bias-variance tradeoff?\"\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"cv\": \"Experience with data visualization and dashboarding.\",\n",
        "        \"jd\": \"Hiring a Data Analyst with expertise in visualization tools.\",\n",
        "        \"expected\": [\n",
        "            \"Data Analyst - Visualization - What are some best practices for designing dashboards?\",\n",
        "            \"Data Analyst - Visualization - What are KPIs and how do you visualize them?\",\n",
        "            \"Data Analyst - Visualization - What is the difference between bar chart and histogram?\"\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"cv\": \"Experience in deep learning and optimization algorithms.\",\n",
        "        \"jd\": \"Seeking an ML Engineer experienced in optimization and training stability.\",\n",
        "        \"expected\": [\n",
        "            \"ML Engineer - Optimization - What is gradient descent?\",\n",
        "            \"ML Engineer - Optimization - What is gradient clipping?\",\n",
        "            \"ML Engineer - Optimization - What is hyperparameter tuning?\"\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"cv\": \"Skilled in SQL query optimization and performance tuning.\",\n",
        "        \"jd\": \"Looking for a Data Engineer who can work with complex SQL queries.\",\n",
        "        \"expected\": [\n",
        "            \"Data Engineer - SQL - What is the difference between UNION and UNION ALL in SQL?\",\n",
        "            \"Data Engineer - SQL - What is the difference between HAVING and WHERE in SQL?\",\n",
        "            \"Data Engineer - SQL - What are window functions used for in SQL?\"\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"cv\": \"Knowledge of statistics, hypothesis testing, and probability.\",\n",
        "        \"jd\": \"Hiring a Data Scientist with strong statistical background.\",\n",
        "        \"expected\": [\n",
        "            \"Data Scientist - Statistics - What is p-value in hypothesis testing?\",\n",
        "            \"Data Scientist - Statistics - What is the central limit theorem?\",\n",
        "            \"Data Scientist - Statistics - What is the difference between Type I and Type II error?\"\n",
        "        ]\n",
        "    }\n",
        "]\n"
      ],
      "metadata": {
        "id": "1tLDRwoF_z74"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from pymilvus import connections, Collection\n",
        "\n",
        "# --- Connect to Zilliz Cloud ---\n",
        "connections.connect(\n",
        "    alias=\"default\",\n",
        "    uri= \"https://in03-feb569ec82b1b76.serverless.aws-eu-central-1.cloud.zilliz.com\",\n",
        "    token=\"520e0e883ef97fcc4663dca8514090a2a491dd29b714711e961807fdbff8a163d82abd308cf1a8ef546698eb1759aa7054cbc295\"\n",
        ")\n",
        "\n",
        "collection = Collection(\"interveiw_Knowledge\")\n",
        "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# --- Helper function to get retrieved questions ---\n",
        "def retrieve_questions_from_zilliz(cv_text, jd_text, top_k=5):\n",
        "    query = f\"{jd_text[:500]} related interview questions for candidate with {cv_text[:500]}\"\n",
        "    query_vector = embedder.encode([query]).tolist()\n",
        "    search_params = {\"metric_type\": \"COSINE\", \"params\": {\"nprobe\": 10}}\n",
        "    results = collection.search(\n",
        "        data=query_vector,\n",
        "        anns_field=\"embedding\",\n",
        "        param=search_params,\n",
        "        limit=top_k,\n",
        "        output_fields=[\"text\"]\n",
        "    )\n",
        "    hits = [hit.entity.get(\"text\") for hit in results[0]]\n",
        "    return hits\n"
      ],
      "metadata": {
        "id": "D4vx8necAku7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_retrieval(predicted, actual):\n",
        "    predicted_set = set(predicted)\n",
        "    actual_set = set(actual)\n",
        "    true_positives = len(predicted_set & actual_set)\n",
        "    precision = true_positives / len(predicted_set) if predicted_set else 0\n",
        "    recall = true_positives / len(actual_set) if actual_set else 0\n",
        "    accuracy = true_positives / max(len(predicted_set | actual_set), 1)\n",
        "    return precision, recall, accuracy\n"
      ],
      "metadata": {
        "id": "YJN5Vk_bAyMm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = []\n",
        "for case in test_cases:\n",
        "    retrieved = retrieve_questions_from_zilliz(case[\"cv\"], case[\"jd\"])\n",
        "    precision, recall, accuracy = evaluate_retrieval(retrieved, case[\"expected\"])\n",
        "    results.append({\n",
        "        \"case\": case[\"jd\"][:40] + \"...\",\n",
        "        \"precision\": round(precision, 2),\n",
        "        \"recall\": round(recall, 2),\n",
        "        \"accuracy\": round(accuracy, 2)\n",
        "    })\n",
        "\n",
        "import pandas as pd\n",
        "df = pd.DataFrame(results)\n",
        "print(df)\n",
        "print(\"Mean Precision:\", df.precision.mean())\n",
        "print(\"Mean Recall:\", df.recall.mean())\n",
        "print(\"Mean Accuracy:\", df.accuracy.mean())\n"
      ],
      "metadata": {
        "id": "zMdesu9WA1li"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation to Measure Semantic Similarity"
      ],
      "metadata": {
        "id": "pOVcsh6QI1Xr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import util\n",
        "\n",
        "def semantic_evaluate(predicted, actual, model, threshold=0.5):\n",
        "    \"\"\"Compute semantic similarity (average cosine) between retrieved and expected questions.\"\"\"\n",
        "    total_score = 0\n",
        "    count = 0\n",
        "    for p in predicted:\n",
        "        for a in actual:\n",
        "            sim = util.cos_sim(model.encode(p), model.encode(a)).item()\n",
        "            total_score += sim\n",
        "            count += 1\n",
        "    avg_similarity = total_score / count if count else 0\n",
        "    return avg_similarity\n",
        "\n",
        "# Example usage:\n",
        "similarities = []\n",
        "for i, case in enumerate(test_cases):\n",
        "    retrieved = retrieve_questions_from_zilliz(case[\"cv\"], case[\"jd\"], top_k=5)\n",
        "    sim = semantic_evaluate(retrieved, case[\"expected\"], embedder)\n",
        "    similarities.append({\"case\": case[\"jd\"][:50], \"avg_similarity\": round(sim, 3)})\n",
        "\n",
        "import pandas as pd\n",
        "df = pd.DataFrame(similarities)\n",
        "display(df)\n",
        "print(\"Mean Semantic Similarity:\", round(df.avg_similarity.mean(), 3))\n"
      ],
      "metadata": {
        "id": "bTOLdhnMHioI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.barh(df[\"case\"], df[\"avg_similarity\"], color=\"skyblue\")\n",
        "plt.xlabel(\"Average Semantic Similarity\")\n",
        "plt.title(\"Retrieval Performance by Job Role\")\n",
        "plt.gca().invert_yaxis()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "MKqKbDm9IDen"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}